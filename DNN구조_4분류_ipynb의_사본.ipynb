{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asdfasdf001234/2024-1-MLPRJ/blob/main/DNN%EA%B5%AC%EC%A1%B0_4%EB%B6%84%EB%A5%98_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UF5xQGj9DqXx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "import seaborn as sns\n",
        "\n",
        "# pretrained 관련\n",
        "import torch\n",
        "import torchvision.transforms as v2\n",
        "from torchvision import models\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def set_random_seed(seed_value):\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    random.seed(seed_value)\n",
        "\n",
        "# Set a random seed value\n",
        "seed_value = 42\n",
        "set_random_seed(seed_value)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZo2AcfiTyTO",
        "outputId": "67c58728-5a90-428f-e679-3f248201869c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset , DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import v2\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "data_dir = \"/content/drive/MyDrive/Colab Notebooks/MLPRJ/Data\""
      ],
      "metadata": {
        "id": "E-DkVabbD4EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "def create_dataframe(data_path, label_list, data_type):\n",
        "    df = pd.DataFrame({\"path\": [], \"label\": [], \"class_id\": []})\n",
        "    img_list = glob(os.path.join(data_path, '*.jpg'))\n",
        "\n",
        "    for img in img_list:\n",
        "      file_name = os.path.splitext(os.path.basename(img))[0]\n",
        "      label_index = int(file_name[0]) - 1  #진영님 코드와 동일\n",
        "      if 0 <= label_index < len(label_list):\n",
        "        label = label_list[label_index]\n",
        "        new_data = pd.DataFrame({\"path\": [img], \"label\": [label], \"class_id\": [label_index]})\n",
        "        df = pd.concat([df, new_data], ignore_index=True)\n",
        "\n",
        "\n",
        "    df[[\"path\"]] = df[[\"path\"]].astype(str)\n",
        "    df[[\"label\"]] = df[[\"label\"]].astype(str)\n",
        "    df[[\"class_id\"]] = df[[\"class_id\"]].astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "'''\n",
        "#웜쿨 분류 코드 (2분류 체계)\n",
        "    for img in img_list:\n",
        "      file_name = os.path.splitext(os.path.basename(img))[0]\n",
        "      label_index = int(file_name[0]) - 1\n",
        "      if label_index == 0 or label_index == 2:   #웜톤\n",
        "        label_index = 0                          #0으로 통일\n",
        "      else:                                      #쿨톤\n",
        "        label_index = 1                          #1으로 통일\n",
        "      if 0 <= label_index < len(label_list):\n",
        "        label = label_list[label_index]\n",
        "        new_data = pd.DataFrame({\"path\": [img], \"label\": [label], \"class_id\": [label_index]})\n",
        "        df = pd.concat([df, new_data], ignore_index=True)\n",
        "#수정 코드 끝\n",
        "'''\n",
        "\n",
        "\n",
        "'''\n",
        "기존 코드 (4분류 체계)\n",
        "    for img in img_list:\n",
        "      file_name = os.path.splitext(os.path.basename(img))[0]\n",
        "      label_index = int(file_name[0]) - 1  #진영님 코드와 동일\n",
        "      if 0 <= label_index < len(label_list):\n",
        "        label = label_list[label_index]\n",
        "        new_data = pd.DataFrame({\"path\": [img], \"label\": [label], \"class_id\": [label_index]})\n",
        "        df = pd.concat([df, new_data], ignore_index=True)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "emQocv1ZD9Jv",
        "outputId": "dd3c2b84-3b0d-4b7e-b67d-c21ae1bd70c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n기존 코드 (4분류 체계)\\n    for img in img_list:\\n      file_name = os.path.splitext(os.path.basename(img))[0]\\n      label_index = int(file_name[0]) - 1  #진영님 코드와 동일\\n      if 0 <= label_index < len(label_list):\\n        label = label_list[label_index]\\n        new_data = pd.DataFrame({\"path\": [img], \"label\": [label], \"class_id\": [label_index]})\\n        df = pd.concat([df, new_data], ignore_index=True)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 기존 example 코드\n",
        "train_path = data_dir + '/train'\n",
        "valid_path = data_dir + '/val'\n",
        "test_path = data_dir + '/test'\n",
        "label_list = ['spring', 'summer', 'autumn', 'winter']\n",
        "#label_list = ['warm', 'cool']\n",
        "\n",
        "train_df = create_dataframe(train_path, label_list, 'training')\n",
        "val_df = create_dataframe(valid_path, label_list, 'validation')\n",
        "test_df = create_dataframe(test_path, label_list, 'testing')"
      ],
      "metadata": {
        "id": "wA3_8kmIECKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train_data: {len(train_df)}\")\n",
        "print(f\"val_data:{len(val_df)}\")\n",
        "print(f\"test_data:{len(test_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WniOgg3IEEQn",
        "outputId": "40f5509c-d303-43cd-a633-a3a4edb47438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_data: 413\n",
            "val_data:137\n",
            "test_data:137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self , dataframe , transforms_):\n",
        "        self.df = dataframe\n",
        "        self.transforms_ = transforms_\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self ,index):\n",
        "        img_path = self.df.iloc[index]['path']\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        transformed_img = self.transforms_(img)\n",
        "        class_id = self.df.iloc[index]['class_id']\n",
        "        return transformed_img , class_id"
      ],
      "metadata": {
        "id": "8WCdOeRjEIpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Transforms = v2.Compose([\n",
        "    v2.Resize((224,224)),\n",
        "    v2.RandomRotation(degrees=10),\n",
        "    v2.RandomHorizontalFlip(p=0.8),\n",
        "    v2.ScaleJitter(target_size=(224,224)),\n",
        "    v2.RandomAffine(degrees=45),\n",
        "    #v2.RandomErasing(),\n",
        "    v2.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.)),\n",
        "    v2.ElasticTransform(alpha=250.0),\n",
        "    #v2.ColorJitter(0.5, 0.5),\n",
        "    #v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
        "\n",
        "    v2.Resize((64,64)), #사이즈를 64*64\n",
        "    v2.PILToTensor(),\n",
        "    v2.ToDtype(torch.float32),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])"
      ],
      "metadata": {
        "id": "gMImJBQJEJ3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 100\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "_YGRg4lwELQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get dataloader\n",
        "\n",
        "train_dataset = BaseDataset(train_df, Transforms) # train_transforms\n",
        "val_dataset = BaseDataset(val_df, Transforms)\n",
        "test_dataset = BaseDataset(test_df, Transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset , batch_size=BATCH_SIZE , shuffle = True)\n",
        "val_loader = DataLoader(val_dataset , batch_size=BATCH_SIZE , shuffle = True)\n",
        "test_loader = DataLoader(test_dataset , batch_size=BATCH_SIZE , shuffle = True)"
      ],
      "metadata": {
        "id": "vzKmNyXgENj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        # 64*64에 맞춰 레이어 수정\n",
        "        self.fc1 = nn.Linear(64*64*3, 10000)\n",
        "        self.fc2 = nn.Linear(10000, 50000)\n",
        "        self.fc3 = nn.Linear(50000, 10000)\n",
        "        self.fc4 = nn.Linear(10000, 5000)\n",
        "        self.fc5 = nn.Linear(5000, 1024)\n",
        "        self.fc6 = nn.Linear(1024, 512)\n",
        "        self.fc7 = nn.Linear(512, 64)\n",
        "        self.fc8 = nn.Linear(64, 32)\n",
        "        self.fc9 = nn.Linear(32, 4)\n",
        "\n",
        "        self.b1 = nn.BatchNorm1d(10000)\n",
        "        self.b2 = nn.BatchNorm1d(50000)\n",
        "        self.b3 = nn.BatchNorm1d(1024)\n",
        "        self.b4 = nn.BatchNorm1d(512)\n",
        "        self.b5 = nn.BatchNorm1d(64)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.b1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.b2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.b1(self.fc3(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.b3(self.fc5(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.b4(self.fc6(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.b5(self.fc7(x)))\n",
        "        x = F.leaky_relu(self.fc8(x))\n",
        "        x = self.fc9(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "eeCNTu4O7ipi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameter 설정\n",
        "import torch.optim as optim\n",
        "FcModel = NeuralNetwork()\n",
        "criterion = nn.CrossEntropyLoss() # loss function\n",
        "optimizer = optim.Adam(FcModel.parameters(), lr=0.0001 )\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "\n",
        "EPOCHS = 20 # the number of epochs\n",
        "n_batch = 32 # the number of batches\n"
      ],
      "metadata": {
        "id": "UJVmK7ikEt51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader , model , loss_fn , optimizer , lr_scheduler=None):\n",
        "    size = 0\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    model.train()\n",
        "    epoch_loss , epoch_correct = 0 , 0\n",
        "\n",
        "    for i ,(data_ , target_) in enumerate(dataloader):\n",
        "        #===================================================#\n",
        "        #모델 예측값과 실제 값\n",
        "        data_, target_ = data_.to(device), target_.to(device)\n",
        "        size += data_.size(0)\n",
        "\n",
        "        pred = model(data_)\n",
        "        loss = loss_fn(pred, target_)\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_correct += ( pred.argmax(1) == target_ ).type(torch.float).sum().item()\n",
        "\n",
        "        #역전파\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        #===================================================#\n",
        "\n",
        "    if lr_scheduler != None:\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    return epoch_correct/size , epoch_loss / num_batches"
      ],
      "metadata": {
        "id": "z_ryDQiHUmAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader , model , loss_fn):\n",
        "    size = 0\n",
        "    num_baches = len(dataloader)\n",
        "    epoch_loss , epoch_correct= 0 ,0\n",
        "\n",
        "    with torch.no_grad(): # grad 연산 X\n",
        "        model.eval() # evaluation dropout 연산시\n",
        "        for i, (data_ , target_) in enumerate(dataloader):\n",
        "\n",
        "            #========================================#\n",
        "            data_, target_ = data_.to(device), target_.to(device)\n",
        "            size += data_.size(0)\n",
        "            pred = model(data_)\n",
        "            loss = criterion(pred, target_)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_correct += ( pred.argmax(1) == target_ ).type(torch.float).sum().item()\n",
        "\n",
        "            #========================================#\n",
        "\n",
        "    return epoch_correct/size  , epoch_loss / num_baches"
      ],
      "metadata": {
        "id": "itd-dZY-Ul3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "akqQobyJUzi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss = 0"
      ],
      "metadata": {
        "id": "l-cUm7U7UzaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FcModel.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSWjEXgmXim7",
        "outputId": "9b251202-0270-4f67-9a34-68b6a3d59ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNetwork(\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (fc1): Linear(in_features=12288, out_features=10000, bias=True)\n",
              "  (fc2): Linear(in_features=10000, out_features=50000, bias=True)\n",
              "  (fc3): Linear(in_features=50000, out_features=10000, bias=True)\n",
              "  (fc4): Linear(in_features=10000, out_features=5000, bias=True)\n",
              "  (fc5): Linear(in_features=5000, out_features=1024, bias=True)\n",
              "  (fc6): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  (fc7): Linear(in_features=512, out_features=64, bias=True)\n",
              "  (fc8): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (fc9): Linear(in_features=32, out_features=4, bias=True)\n",
              "  (b1): BatchNorm1d(10000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (b2): BatchNorm1d(50000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (b3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (b4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (b5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    train_acc , train_loss = train(train_loader ,\n",
        "                                   FcModel ,\n",
        "                                   criterion ,\n",
        "                                   optimizer,\n",
        "                                   lr_scheduler )\n",
        "\n",
        "    val_acc , val_loss = test(val_loader , FcModel , criterion)\n",
        "    print(f'epoch:{epoch} \\\n",
        "    train_loss = {train_loss:.4f} , train_acc:{train_acc:.4f} \\\n",
        "    val_loss = {val_loss:.4f} , val_acc:{val_acc:.4f} \\\n",
        "    learning rate: {optimizer.param_groups[0][\"lr\"]}')\n",
        "\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "        counter = 0\n",
        "        best_loss = val_loss\n",
        "        torch.save(FcModel.state_dict() , \"checkpoints/NN_best.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccC5RiGAUls6",
        "outputId": "65cf235f-b428-44a9-eb3f-98765cac0369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 1/20 [00:54<17:10, 54.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0     train_loss = 1.3626 , train_acc:0.3123     val_loss = 1.3885 , val_acc:0.3139     learning rate: 9e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 2/20 [01:12<09:54, 33.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1     train_loss = 1.3756 , train_acc:0.3148     val_loss = 1.3838 , val_acc:0.3139     learning rate: 8.1e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 3/20 [01:28<07:13, 25.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:2     train_loss = 1.3675 , train_acc:0.3220     val_loss = 1.3774 , val_acc:0.3139     learning rate: 7.290000000000001e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 4/20 [01:46<05:56, 22.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:3     train_loss = 1.3602 , train_acc:0.3148     val_loss = 1.3682 , val_acc:0.3139     learning rate: 6.561000000000002e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 5/20 [02:04<05:10, 20.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:4     train_loss = 1.3598 , train_acc:0.3196     val_loss = 1.3688 , val_acc:0.3139     learning rate: 5.904900000000002e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 6/20 [02:22<04:36, 19.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:5     train_loss = 1.3631 , train_acc:0.3099     val_loss = 1.3764 , val_acc:0.3139     learning rate: 5.314410000000002e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▌      | 7/20 [02:40<04:09, 19.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:6     train_loss = 1.3609 , train_acc:0.3099     val_loss = 1.3668 , val_acc:0.3139     learning rate: 4.782969000000002e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 8/20 [02:56<03:39, 18.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:7     train_loss = 1.3734 , train_acc:0.3172     val_loss = 1.3660 , val_acc:0.3139     learning rate: 4.304672100000002e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 9/20 [03:14<03:21, 18.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:8     train_loss = 1.3626 , train_acc:0.3123     val_loss = 1.3672 , val_acc:0.3139     learning rate: 3.874204890000002e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 10/20 [03:33<03:04, 18.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:9     train_loss = 1.3684 , train_acc:0.2978     val_loss = 1.3706 , val_acc:0.3139     learning rate: 3.4867844010000016e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 11/20 [03:51<02:43, 18.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:10     train_loss = 1.3626 , train_acc:0.3245     val_loss = 1.3647 , val_acc:0.3139     learning rate: 3.138105960900002e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 12/20 [04:09<02:25, 18.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:11     train_loss = 1.3508 , train_acc:0.3148     val_loss = 1.3629 , val_acc:0.3139     learning rate: 2.8242953648100018e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 13/20 [04:27<02:06, 18.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:12     train_loss = 1.3791 , train_acc:0.3196     val_loss = 1.3660 , val_acc:0.3139     learning rate: 2.5418658283290016e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 14/20 [04:45<01:48, 18.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:13     train_loss = 1.3467 , train_acc:0.2978     val_loss = 1.3634 , val_acc:0.3139     learning rate: 2.2876792454961016e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 15/20 [05:03<01:29, 17.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:14     train_loss = 1.3640 , train_acc:0.3075     val_loss = 1.3692 , val_acc:0.3139     learning rate: 2.0589113209464913e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 16/20 [05:21<01:11, 17.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:15     train_loss = 1.3484 , train_acc:0.3220     val_loss = 1.3706 , val_acc:0.3139     learning rate: 1.8530201888518422e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 17/20 [05:39<00:53, 17.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:16     train_loss = 1.3584 , train_acc:0.3027     val_loss = 1.3667 , val_acc:0.3139     learning rate: 1.667718169966658e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 18/20 [05:57<00:36, 18.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:17     train_loss = 1.3626 , train_acc:0.3075     val_loss = 1.3713 , val_acc:0.3139     learning rate: 1.5009463529699922e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▌| 19/20 [06:15<00:17, 18.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:18     train_loss = 1.3470 , train_acc:0.3027     val_loss = 1.3688 , val_acc:0.3139     learning rate: 1.350851717672993e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [06:32<00:00, 19.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:19     train_loss = 1.3727 , train_acc:0.3366     val_loss = 1.3673 , val_acc:0.3139     learning rate: 1.2157665459056937e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc , val_loss = test(test_loader , FcModel , criterion)\n",
        "print(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0TUfJ1MZKTh",
        "outputId": "34c4cb54-ddf5-4340-8d07-76292af7a493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.31386861313868614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kgw12oU1U84Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}